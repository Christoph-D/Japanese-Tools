[general]
default_model = "deepseek-chat"
enable_compiler_explorer = true
# Timeout for non-reasoning models in seconds
timeout = 20
# Timeout for reasoning models in seconds
timeout_reasoning = 40

[providers.anthropic]
models = [
    { id = "claude-sonnet-4-5", short_name = "s", name = "Claude Sonnet 4.5" }
]

[providers.deepseek]
models = [
    { id = "deepseek-chat", short_name = "d", name = "DeepSeek-V3.1" }
]

[providers.litellm]
endpoint = "http://..."
models = [
    # The short names here collide with the other examples.
    # In a real config, you need to make sure the short names are unique.
    # { id = "deepseek/deepseek-chat", short_name = "d", name = "DeepSeek-V3.2" },
    # { id = "openrouter/google/gemini-3-flash-preview", short_name = "g", name = "Gemini 3 Flash" },
    # { id = "mistral/mistral-medium-latest", short_name = "m", name = "Mistral Medium" },
    # { id = "anthropic/claude-sonnet-4-5", short_name = "s", name = "Claude Sonnet 4.5" }
]

[providers.mistral]
models = [
    { id = "mistral-medium-latest", short_name = "m", name = "Mistral Medium" }
]

[providers.openrouter]
models = [
    { id = "google/gemini-3-flash-preview", short_name = "g", name = "Gemini 3 Flash" },
]

[providers.z-ai]
models = [
    { id = "glm-4.5-air", short_name = "z", name = "GLM-4.5-air", reasoning = true },
]

[providers.z-ai-code]
models = [
    { id = "glm-4.5-air", short_name = "c", name = "GLM-4.5-air", reasoning = true },
]

[channels."#example"]
default_model = "mistral/mistral-medium-latest"
temperature = 0.4
system_prompt = """You are a technical AI assistant in the #example channel.
Focus on practical, accurate solutions for Linux system administration and development.
Be concise and precise. Limit responses to {MAX_LINE_LENGTH} characters."""
