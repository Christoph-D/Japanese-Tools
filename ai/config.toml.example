[general]
default_model = "deepseek-chat"
enable_compiler_explorer = true
# Timeout for non-reasoning models in seconds
timeout = 20
# Timeout for reasoning models in seconds
timeout_reasoning = 40
# Optional: Maximum tokens for non-reasoning models (default: 500)
# max_tokens = 500
# Optional: Maximum tokens for reasoning models (default: 4096)
# max_tokens_with_reasoning = 4096

[providers.anthropic]
models = [
    {
        # The model name sent to the API
        id = "claude-sonnet-4-5"
        # The flag -<short_name> to select the model (e.g. -s in this example)
        short_name = "s"
        # User-visible name shown in the help string
        name = "Claude Sonnet 4.5"
        # Optional (defaults to false): Set to true to increase timeout/token limit
        reasoning = false
        # Optional (defaults to global config value): The output token limit
        max_tokens = 1024
        # Optional (defaults to global config value): Timeout in seconds
        timeout = 30
    }
]

[providers.deepseek]
models = [
    { id = "deepseek-chat", short_name = "d", name = "DeepSeek-V3.1" }
]

[providers.litellm]
endpoint = "http://..."
models = [
    # The short names here collide with the other examples.
    # In a real config, you need to make sure the short names are unique.
    # { id = "deepseek/deepseek-chat", short_name = "d", name = "DeepSeek-V3.2" },
    # { id = "openrouter/google/gemini-3-flash-preview", short_name = "g", name = "Gemini 3 Flash" },
    # { id = "mistral/mistral-medium-latest", short_name = "m", name = "Mistral Medium" },
    # { id = "anthropic/claude-sonnet-4-5", short_name = "s", name = "Claude Sonnet 4.5" }
]

[providers.mistral]
models = [
    { id = "mistral-medium-latest", short_name = "m", name = "Mistral Medium" }
]

[providers.openrouter]
models = [
    { id = "google/gemini-3-flash-preview", short_name = "g", name = "Gemini 3 Flash" },
]

[providers.z-ai]
models = [
    # Example: override max_tokens for this specific model (takes precedence over global settings)
    { id = "glm-4.5-air", short_name = "z", name = "GLM-4.5-air", reasoning = true, max_tokens = 1024 }
]

[providers.z-ai-code]
models = [
    # Example: override timeout for this specific model (takes precedence over global settings)
    { id = "glm-4.5-air", short_name = "c", name = "GLM-4.5-air", reasoning = true, timeout = 60 },
    # Example: override temperature for this specific model (takes precedence over channel settings)
    # { id = "glm-4.5-air", short_name = "c", name = "GLM-4.5-air", temperature = 0.3 },
]

[channels."#example"]
default_model = "mistral/mistral-medium-latest"
system_prompt = """You are a technical AI assistant in the #example channel.
Focus on practical, accurate solutions for Linux system administration and development.
Be concise and precise. Limit responses to {MAX_LINE_LENGTH} characters."""

# Example: override temperature for deepseek/deepseek-chat in #example channel
# [channels."#example".models."deepseek/deepseek-chat"]
# temperature = 0.4
# timeout = 30
